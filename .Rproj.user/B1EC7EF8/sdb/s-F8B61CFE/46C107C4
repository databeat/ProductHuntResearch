{
    "collab_server" : "",
    "contents" : "############################\n## COURSE 4, TASK 4 MODELING\n############################\n\nwifi <- read.csv(\"./UJIndoorLoc/trainingData.csv\", header = TRUE)\nwifi_newData <- read.csv(\"./UJIndoorLoc/validationData.csv\", header = TRUE)\nstr(wifi, list.len = 529)\nstr(wifi_newData, list.len = 529)\nsummary(wifi)\n# The following attributes have 100 for every observation\n  # WAP003\n  # WAP004\n  # WAP092\n  # WAP093\n  # WAP094\n  # WAP095\n  # WAP152\n  # WAP158\n  # WAP159\n  # WAP160\n  # WAP215\n  # WAP217\n  # WAP226\n  # WAP227\n  # WAP238\n  # WAP239\n  # WAP240\n  # WAP241\n  # WAP242\n  # WAP243\n  # WAP244\n  # WAP245\n  # WAP246\n  # WAP247\n  # WAP254\n  # WAP293\n  # WAP296\n  # WAP301\n  # WAP303\n  # WAP304\n  # WAP307\n  # WAP333\n  # WAP349\n  # WAP353\n  # WAP360\n  # WAP365\n  # WAP416\n  # WAP419\n  # WAP423\n  # WAP429\n  # WAP433\n  # WAP438\n  # WAP441\n  # WAP442\n  # WAP444\n  # WAP445\n  # WAP451\n  # WAP458\n  # WAP482\n  # WAP485\n  # WAP487\n  # WAP488\n  # WAP491\n  # WAP497\n  # WAP520\n\nlibrary(dplyr)\nwifi_clean <- select(wifi, -WAP003, -WAP004, -WAP092, -WAP093, -WAP094, -WAP095, -WAP152, -WAP158, -WAP159, \n                     -WAP160, -WAP215, -WAP217, -WAP226, -WAP227, -WAP238, -WAP239, -WAP240, -WAP241, -WAP242, \n                     -WAP243, -WAP244, -WAP245, -WAP246, -WAP247, -WAP254, -WAP293, -WAP296, -WAP301, -WAP303, \n                     -WAP304, -WAP307, -WAP333, -WAP349, -WAP353, -WAP360, -WAP365, -WAP416, -WAP419, -WAP423, \n                     -WAP429, -WAP433, -WAP438, -WAP441, -WAP442, -WAP444, -WAP445, -WAP451, -WAP458, -WAP482, \n                     -WAP485, -WAP487, -WAP488, -WAP491, -WAP497, -WAP520, -LONGITUDE, -LATITUDE, -USERID, \n                     -PHONEID, -TIMESTAMP, -RELATIVEPOSITION)\nbuilding0 <- filter(wifi_clean, BUILDINGID == 0)\nbuilding1 <- filter(wifi_clean, BUILDINGID == 1)\nbuilding2 <- filter(wifi_clean, BUILDINGID == 2)\n\nbuilding0 <- select(building0, -BUILDINGID)\nbuilding1 <- select(building1, -BUILDINGID)\nbuilding2 <- select(building2, -BUILDINGID)\n\n################\n## BUILDING 0 ##\n################\n\nstr(building0, list.len = 467)\n\nfor (i in 1:nrow(building0)) {\n  if(building0$SPACEID[i] >= 101 && building0$SPACEID[i] <= 140) {\n    building0$SPACEID[i] <- building0$SPACEID[i]%%100\n  }\n  else if(building0$SPACEID[i] >= 201 && building0$SPACEID[i] <= 237) {\n    building0$SPACEID[i] <- building0$SPACEID[i] - 160\n  }\n  else if(building0$SPACEID[i] == 241) {\n    building0$SPACEID[i] == 81\n  }\n}\nbuilding0$SPACEID\n\nbuilding0$FLOOR <- as.factor(building0$FLOOR)\nbuilding0$SPACEID <- as.factor(building0$SPACEID)\nstr(building0, list.len = 467)\n\n## Feature selection for SPACEID in Building 0\nlibrary(FSelector)\nbuilding0_weights <- information.gain(SPACEID~., building0)\nbuilding0_chisquare <- chi.squared(SPACEID~., building0)\narrange(building0_weights, desc(attr_importance))\narrange(building0_chisquare, desc(attr_importance))\nbuilding0_weight_cutoffs <- cutoff.k(building0_weights, 106)\nbuilding0_chisquare_cutoffs <- cutoff.k(building0_chisquare, 106)\nsetdiff(building0_weight_cutoffs, building0_chisquare_cutoffs)\n  # both chi squared and info gain selected same 106 attributes\n\nbuilding0_formula <- as.simple.formula(building0_weight_cutoffs, \"SPACEID\")\n\n\n## create training and testing sets using 80/20 split\nlibrary(caret)\nset.seed(777)\nbuilding0_inTrain <- createDataPartition(y = building0$SPACEID, p = 0.8, list = FALSE)\nbuilding0_training <- building0[building0_inTrain, ]\nbuilding0_testing <- building0[-building0_inTrain, ]\n\n\n###########\n## MODELING\n###########\nnames(getModelInfo())\n\n##### DECISION TREES\ntree_fitControl <- trainControl(method = \"cv\", number = 10)\nbuilding0_treeFit <- train(building0_formula, data = building0_training, method = \"C5.0\", trControl = tree_fitControl)\nbuilding0_treeFit\n  # Accuracy was used to select the optimal model using  the largest value.\n  # The final values used for the model were trials = 20, model = rules and winnow = TRUE. \n    # model  winnow  trials  Accuracy   Kappa\n    # rules   TRUE   20      0.6839517  0.6794865\n  # Doesn't seem that good\nbuilding0_treePred <- predict(building0_treeFit, building0_testing)\npostResample(building0_treePred, building0_testing$SPACEID)\n# Accuracy    Kappa \n# 0.7125984   0.7085084\n\n\n##### RANDOM FOREST\nrf_fitControl <- trainControl(method = \"cv\", number = 10)\nbuilding0_rfFit <- train(building0_formula, data = building0_training, method = \"rf\", trControl = rf_fitControl)\nbuilding0_rfFit\n  # Accuracy was used to select the optimal model using  the largest value.\n  # The final value used for the model was mtry = 55.\n    # mtry  Accuracy   Kappa    \n    # 55    0.7467612  0.7431769\nbuilding0_rfPred <- predict(building0_rfFit, building0_testing)\npostResample(building0_rfPred, building0_testing$SPACEID)\n# Accuracy    Kappa \n# 0.7726378   0.7694040 \n\n\n##### SVM\nsvm_fitControl <- trainControl(method = \"cv\", number = 10)\nbuilding0_svmFit <- train(building0_formula, data = building0_training, method = \"svmRadial\", trControl = svm_fitControl)\nbuilding0_svmFit\n  # Tuning parameter 'sigma' was held constant at a value of 0.006352486\n  # Accuracy was used to select the optimal model using  the largest value.\n  # The final values used for the model were sigma = 0.006352486 and C = 1. \n    # C     Accuracy   Kappa\n    # 1.00  0.4852169  0.4777188\nbuilding0_svmPred <- predict(building0_svmFit, building0_testing)\npostResample(building0_svmPred, building0_testing$SPACEID)\n  # Accuracy    Kappa \n  # 0.4960630   0.4887689 \n\n\n##### IBk (kNearestNeighbors)\nlibrary(RWeka)\nbuilding0_ibkFit <- IBk(building0_formula, data = building0_training, control = Weka_control(K = 100, X = TRUE))\nbuilding0_ibkFit\n# IB1 instance-based classifier\n# using 1 nearest neighbour(s) for classification\nbuilding0_ibkFit_inverse <- IBk(building0_formula, data = building0_training, control = Weka_control(K = 1, I = TRUE))\nbuilding0_ibkFit_similar <- IBk(building0_formula, data = building0_training, control = Weka_control(K = 1, F = TRUE))\nbuilding0_ibkFit_regular <- IBk(building0_formula, data = building0_training, control = Weka_control(K = 1))\nevaluate_Weka_classifier(building0_ibkFit_inverse, numFolds = 10)\n  # Accuracy    Kappa\n  # 0.594614    0.5889\nevaluate_Weka_classifier(building0_ibkFit_similar, numFolds = 10)\n  # Accuracy    Kappa\n  # 0.59863     0.593\nevaluate_Weka_classifier(building0_ibkFit_regular, numFolds = 10)\n  # Accuracy    Kappa\n  # 0.596031    0.5903\nbuilding0_ibkPred <- predict(building0_ibkFit_similar, building0_testing)\npostResample(building0_ibkPred, building0_testing$SPACEID)\n  # Accuracy    Kappa \n  # 0.6043307   0.5987590 \n\n\n##### Bagged AdaBoost\nadaBag_fitControl <- trainControl(method = \"cv\", number = 10)\nbuilding0_adaBagFit <- train(building0_formula, data = building0_training, method = \"AdaBag\", trControl = adaBag_fitControl)\nbuilding0_adaBagFit\n  # Accuracy was used to select the optimal model using  the largest value.\n  # The final values used for the model were mfinal = 100 and maxdepth = 3. \nbuilding0_adaBagPred <- predict(building0_adaBagFit, building0_testing)\npostResample(building0_adaBagPred, building0_testing$SPACEID)\n# Accuracy      Kappa \n# 0.07972441    0.06341444 \n\n\n##### Neural Network\nnn_fitControl <- trainControl(method = \"cv\", number = 10)\nbuilding0_nnFit <- train(building0_formula, data = building0_training, method = \"avNNet\", trControl = nn_fitControl, MaxNWts = 1013)\nbuilding0_nnFit\nbuilding0_nnPred <- predict(building0_nnFit, building0_testing)\npostResample(building0_nnPred, building0_testing$SPACEID)\n# Accuracy      Kappa \n# 0.06496063    0.04842466\n\n\n##### Boosted Logistic Regression\nblr_fitControl <- trainControl(method = \"cv\", number = 10)\nbuilding0_blrFit <- train(building0_formula, data = building0_training, method = \"LogitBoost\", trControl = blr_fitControl)\nbuilding0_blrFit\n  # Accuracy was used to select the optimal model using  the largest value.\n  # The final value used for the model was nIter = 31. \n    # nIter  Accuracy   Kappa    \n    # 11     0.5054878  0.4977801\n    # 21     0.6168501  0.6110588\n    # 31     0.6410236  0.6356558\n  # Hasn't maxed out yet.\nbuilding0_blrPred <- predict(building0_blrFit, building0_testing)\npostResample(building0_blrPred, building0_testing$SPACEID)\n  # Accuracy    Kappa \n  # 0.6879643   0.6834296\n# Try higher nIter\nbuilding0_blrFit2 <- train(building0_formula, data = building0_training, method = \"LogitBoost\", trControl = blr_fitControl, nIter = 41)\nbuilding0_blrFit2\n  # Didn't work; didn't change nIter\nblrGrid <-  expand.grid(nIter = c(11, 21, 31, 41, 51, 61, 71, 81))\nbuilding0_blrFit2 <- train(building0_formula, data = building0_training, method = \"LogitBoost\", trControl = blr_fitControl, tuneGrid = blrGrid)\nbuilding0_blrFit2\n  # Still hasn't maxed out, but doesn't look to be going much higher than 0.69 accuracy\nblrGrid <-  expand.grid(nIter = c(81, 86, 91, 96, 101, 106))\nbuilding0_blrFit2 <- train(building0_formula, data = building0_training, method = \"LogitBoost\", trControl = blr_fitControl, tuneGrid = blrGrid)\nbuilding0_blrFit2\nbuilding0_blrPred2 <- predict(building0_blrFit2, building0_testing)\npostResample(building0_blrPred2, building0_testing$SPACEID)\n  # Accuracy    Kappa\n  # 0.7273839   0.7234937\n  # Best performance with the Boosted Logistic Regression still isn't as good as first Random Forest\n\n\n##### RANDOM FOREST THE RETURN\nrf_fitControl <- trainControl(method = \"cv\", number = 10)\nrfGrid1 <- expand.grid(mtry = c(45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105))\nbuilding0_rfFit2 <- train(building0_formula, data = building0_training, method = \"rf\", trControl = rf_fitControl, tuneGrid = rfGrid1)\nbuilding0_rfFit2\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was mtry = 50\n# mtry  Accuracy   Kappa    \n# 50    0.7523633  0.7488503\nrfGrid2 <- expand.grid(mtry = c(46, 47, 48, 49, 50, 51, 52, 53, 54))\nbuilding0_rfFit3 <- train(building0_formula, data = building0_training, method = \"rf\", trControl = rf_fitControl, tuneGrid = rfGrid2)\nbuilding0_rfFit3\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was mtry = 48. \n# mtry  Accuracy   Kappa\n# 48    0.7545173  0.7510433\nbuilding0_rfPred2 <- predict(building0_rfFit3, building0_testing)\npostResample(building0_rfPred2, building0_testing$SPACEID)\n# Accuracy    Kappa \n# 0.7746063   0.7714043 \n  # Best accuracy I've been able to achieve, and it's not going much higher. I think I'll use this model\n\nbuilding0_treeFit <- NULL\nbuilding0_svmFit <- NULL\nbuilding0_ibkFit <- NULL\nbuilding0_ibkFit_inverse <- NULL\nbuilding0_ibkFit_similar <- NULL\nbuilding0_ibkFit_regular <- NULL\nbuilding0_nnFit <- NULL\nbuilding0_adaBagFit <- NULL\nbuilding0_blrFit <- NULL\nbuilding0_blrFit2 <- NULL\nbuilding0_rfFit <- NULL\nbuilding0_rfFit2 <- NULL\n\n\n\n################\n## BUILDING 1 ##\n################\n\nstr(building1, list.len = 467)\n\nfor (i in 1:nrow(building1)) {\n  if(building1$SPACEID[i] == 22) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 3\n  }\n  else if(building1$SPACEID[i] >= 25 && building1$SPACEID[i] <= 30) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 5\n  }\n  else if(building1$SPACEID[i] >= 101 && building1$SPACEID[i] <= 119) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 75\n  }\n  else if(building1$SPACEID[i] >= 121 && building1$SPACEID[i] <= 122) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 76\n  }\n  else if(building1$SPACEID[i] >= 201 && building1$SPACEID[i] <= 220) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 154\n  }\n  else if(building1$SPACEID[i] == 222) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 155\n  }\n  else if(building1$SPACEID[i] >= 224 && building1$SPACEID[i] <= 231) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 156\n  }\n  else if(building1$SPACEID[i] == 235) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 159\n  }\n  else if(building1$SPACEID[i] >= 237 && building1$SPACEID[i] <= 239) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 160\n  }\n  else if(building1$SPACEID[i] >= 243 && building1$SPACEID[i] <= 249) {\n    building1$SPACEID[i] <- building1$SPACEID[i] - 163\n  }\n}\nbuilding1$SPACEID\n\nbuilding1$FLOOR <- as.factor(building1$FLOOR)\nbuilding1$SPACEID <- as.factor(building1$SPACEID)\nstr(building1, list.len = 467)\n\n\n## Feature selection for SPACEID in Building 1\nlibrary(FSelector)\nbuilding1_weights <- information.gain(SPACEID~., building1)\nbuilding1_chisquare <- chi.squared(SPACEID~., building1)\narrange(building1_weights, desc(attr_importance))\narrange(building1_chisquare, desc(attr_importance))\nbuilding1_weight_cutoffs <- cutoff.k(building1_weights, 157)\nbuilding1_chisquare_cutoffs <- cutoff.k(building1_chisquare, 157)\nsetdiff(building1_weight_cutoffs, building1_chisquare_cutoffs)\n  # both chi squared and info gain selected same 157 attributes\nbuilding1_formula <- as.simple.formula(building1_weight_cutoffs, \"SPACEID\")\n\n## create training and testing sets using 80/20 split\nlibrary(caret)\nset.seed(777)\nbuilding1_inTrain <- createDataPartition(y = building1$SPACEID, p = 0.8, list = FALSE)\nbuilding1_training <- building1[building1_inTrain, ]\nbuilding1_testing <- building1[-building1_inTrain, ]\n\n\n###########\n## MODELING\n###########\n\n##### RANDOM FOREST\nrf_fitControl <- trainControl(method = \"cv\", number = 10)\nbuilding1_rfFit <- train(building1_formula, data = building1_training, method = \"rf\", trControl = rf_fitControl)\nbuilding1_rfFit\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was mtry = 80\n# mtry  Accuracy   Kappa    \n# 80    0.8574332  0.8541609\nbuilding1_rfPred <- predict(building1_rfFit, building1_testing)\npostResample(building1_rfPred, building1_testing$SPACEID)\n# Accuracy    Kappa \n# 0.8699507   0.8669269 \n\nbuilding1_rfGrid <- expand.grid(mtry = c(40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145))\nbuilding1_rfFit2 <- train(building1_formula, data = building1_training, method = \"rf\", trControl = rf_fitControl, tuneGrid = building1_rfGrid)\nbuilding1_rfFit2\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was mtry = 40. \n# mtry  Accuracy   Kappa\n# 40   0.8632020  0.8600763\nbuilding1_rfGrid2 <- expand.grid(mtry = c(20, 25, 30, 35, 40, 45))\nbuilding1_rfFit3 <- train(building1_formula, data = building1_training, method = \"rf\", trControl = rf_fitControl, tuneGrid = building1_rfGrid2)\nbuilding1_rfFit3\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was mtry =  25\n# mtry  Accuracy   Kappa\n# 25    0.8614835  0.8583103\nbuilding1_rfGrid3 <- expand.grid(mtry = c(21, 22, 23, 24, 25, 26, 27, 28, 29))\nbuilding1_rfFit4 <- train(building1_formula, data = building1_training, method = \"rf\", trControl = rf_fitControl, tuneGrid = building1_rfGrid3)\nbuilding1_rfFit4\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was mtry =  26\n# mtry  Accuracy   Kappa\n# 26    0.8649296  0.8618338\nbuilding1_rfPred2 <- predict(building1_rfFit4, building1_testing)\npostResample(building1_rfPred2, building1_testing$SPACEID)\n# Accuracy    Kappa \n# 0.8689655 0.8659165\n\n\n## building1_rfFit actually had marginally better accuracy on the test set\n## than building1_rfFit4 after all the tuning. \n\nbuilding1_rfFit2 <- NULL\nbuilding1_rfFit3 <- NULL\nbuilding1_rfFit4 <- NULL\n\n################\n## BUILDING 2 ##\n################\n\nstr(building2, list.len = 467)\n\nfor (i in 1:nrow(building2)) {\n  if(building2$SPACEID[i] >= 101 && building2$SPACEID[i] <= 144) {\n    building2$SPACEID[i] <- building2$SPACEID[i] - 100\n  }\n  else if(building2$SPACEID[i] >= 146 && building2$SPACEID[i] <= 147) {\n    building2$SPACEID[i] <- building2$SPACEID[i] - 101\n  }\n  else if(building2$SPACEID[i] >= 201 && building2$SPACEID[i] <= 248) {\n    building2$SPACEID[i] <- building2$SPACEID[i] - 154\n  }\n  else if(building2$SPACEID[i] == 250) {\n    building2$SPACEID[i] <- building2$SPACEID[i] - 155\n  }\n  else if(building2$SPACEID[i] == 253 || building1$SPACEID[i] == 254) {\n    building2$SPACEID[i] <- building2$SPACEID[i] - 157\n  }\n}\nbuilding2$SPACEID\n\nbuilding2$FLOOR <- as.factor(building2$FLOOR)\nbuilding2$SPACEID <- as.factor(building2$SPACEID)\nstr(building2, list.len = 467)\n\n\n## Feature selection for SPACEID in Building 1\nlibrary(FSelector)\nbuilding2_weights <- information.gain(SPACEID~., building2)\nbuilding2_chisquare <- chi.squared(SPACEID~., building2)\narrange(building2_weights, desc(attr_importance))\narrange(building2_chisquare, desc(attr_importance))\nbuilding2_weight_cutoffs <- cutoff.k(building2_weights, 135)\nbuilding2_chisquare_cutoffs <- cutoff.k(building2_chisquare, 135)\nsetdiff(building2_weight_cutoffs, building2_chisquare_cutoffs)\n# both chi squared and info gain selected same 157 attributes\nbuilding2_formula <- as.simple.formula(building2_weight_cutoffs, \"SPACEID\")\n\n## create training and testing sets using 80/20 split\nlibrary(caret)\nset.seed(777)\nbuilding2_inTrain <- createDataPartition(y = building2$SPACEID, p = 0.8, list = FALSE)\nbuilding2_training <- building2[building2_inTrain, ]\nbuilding2_testing <- building2[-building2_inTrain, ]\n\n\n###########\n## MODELING\n###########\n\n##### RANDOM FOREST\nrf_fitControl <- trainControl(method = \"cv\", number = 10)\nbuilding2_rfFit <- train(building2_formula, data = building2_training, method = \"rf\", trControl = rf_fitControl)\nbuilding2_rfFit\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was mtry = 70\n# mtry  Accuracy   Kappa    \n# 70   0.8142783  0.8112818\nbuilding2_rfPred <- predict(building2_rfFit, building2_testing)\npostResample(building2_rfPred, building2_testing$SPACEID)\n# Accuracy    Kappa \n# 0.8222459   0.8194044\n\nbuilding2_rfGrid <- expand.grid(mtry = c(25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130))\nbuilding2_rfFit2 <- train(building2_formula, data = building2_training, method = \"rf\", trControl = rf_fitControl, tuneGrid = building2_rfGrid)\nbuilding2_rfFit2\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was mtry = 25\n# mtry  Accuracy   Kappa\n# 25   0.8275059  0.8247255\nbuilding2_rfGrid2 <- expand.grid(mtry = c(11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25))\nbuilding2_rfFit3 <- train(building2_formula, data = building2_training, method = \"rf\", trControl = rf_fitControl, tuneGrid = building2_rfGrid2)\nbuilding2_rfFit3\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was mtry = 25\n# mtry  Accuracy   Kappa\n# 25    0.8315892  0.8288753\nbuilding2_rfPred2 <- predict(building2_rfFit3, building2_testing)\npostResample(building2_rfPred2, building2_testing$SPACEID)\n# Accuracy    Kappa \n# 0.8355508 0.8328893\n\n  # Best accuracy. I'll use this one.\n\nbuilding2_rfFit <- NULL\nbuilding2_rfFit2 <- NULL\n\n###################\n### PREDICTIONS ###\n###################\n\nstr(wifi_newData, list.len = 529)\nsummary(wifi_newData)\n\nlibrary(dplyr)\nwifi_newData_clean <- select(wifi_newData, -WAP003, -WAP004, -WAP092, -WAP093, -WAP094, -WAP095, -WAP152,\n                      -WAP158, -WAP159, -WAP160, -WAP215, -WAP217, -WAP226, -WAP227, -WAP238, -WAP239,\n                      -WAP240, -WAP241, -WAP242, -WAP243, -WAP244, -WAP245, -WAP246, -WAP247, -WAP254,\n                      -WAP293, -WAP296, -WAP301, -WAP303, -WAP304, -WAP307, -WAP333, -WAP349, -WAP353,\n                      -WAP360, -WAP365, -WAP416, -WAP419, -WAP423, -WAP429, -WAP433, -WAP438, -WAP441,\n                      -WAP442, -WAP444, -WAP445, -WAP451, -WAP458, -WAP482, -WAP485, -WAP487, -WAP488,\n                      -WAP491, -WAP497, -WAP520, -LONGITUDE, -LATITUDE, -USERID, -PHONEID, -TIMESTAMP,\n                      -RELATIVEPOSITION)\nbuilding0_new <- filter(wifi_newData_clean, BUILDINGID == 0)\nbuilding1_new <- filter(wifi_newData_clean, BUILDINGID == 1)\nbuilding2_new <- filter(wifi_newData_clean, BUILDINGID == 2)\n\nbuilding0_new <- select(building0_new, -BUILDINGID)\nbuilding1_new <- select(building1_new, -BUILDINGID)\nbuilding2_new <- select(building2_new, -BUILDINGID)\n\n\n################\n## BUILDING 0 ##\n################\n\nstr(building0_new, list.len = 467)\nbuilding0_new$FLOOR <- as.factor(building0_new$FLOOR)\nstr(building0_new, list.len = 467)\n\nbuilding0_new_pred <- predict(building0_rfFit3, newdata = building0_new)\nbuilding0_new_pred\nwrite.csv(building0_new_pred, file = \"building0_pred.csv\")\n\n################\n## BUILDING 1 ##\n################\n\nstr(building1_new, list.len = 467)\nbuilding1_new$FLOOR <- as.factor(building1_new$FLOOR)\nstr(building1_new, list.len = 467)\n\nbuilding1_new_pred <- predict(building1_rfFit, newdata = building1_new)\nbuilding1_new_pred\nwrite.csv(building1_new_pred, file = \"building1_pred.csv\")\n\n################\n## BUILDING 2 ##\n################\n\nstr(building2_new, list.len = 467)\nbuilding2_new$FLOOR <- as.factor(building2_new$FLOOR)\nstr(building2_new, list.len = 467)\n\nbuilding2_new_pred <- predict(building2_rfFit3, newdata = building2_new)\nbuilding2_new_pred\nwrite.csv(building2_new_pred, file = \"building2_pred.csv\")\n",
    "created" : 1480388588118.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2640868521",
    "id" : "46C107C4",
    "lastKnownWriteTime" : 1480386864,
    "last_content_update" : 1480386864,
    "path" : "~/Google Drive/UT Data Analytics Program/Course 4/Task 4/Course 4 Task 4 modeling.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}